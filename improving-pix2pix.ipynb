{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f26d137b-680f-4235-84dc-a986c0496c76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:05.811738Z",
     "iopub.status.busy": "2022-02-25T20:46:05.811738Z",
     "iopub.status.idle": "2022-02-25T20:46:09.986419Z",
     "shell.execute_reply": "2022-02-25T20:46:09.986419Z",
     "shell.execute_reply.started": "2022-02-25T20:46:05.811738Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import io_utils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "print(\"Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84883266-db03-48aa-832e-ed60f5b10268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:09.988359Z",
     "iopub.status.busy": "2022-02-25T20:46:09.987334Z",
     "iopub.status.idle": "2022-02-25T20:46:11.448487Z",
     "shell.execute_reply": "2022-02-25T20:46:11.447472Z",
     "shell.execute_reply.started": "2022-02-25T20:46:09.988359Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "    print(\"Default GPU: {}\".format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Not using a GPU - it will take long!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e5c88ab-c8ca-46b2-bbd4-f7ef3244b01a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:11.449347Z",
     "iopub.status.busy": "2022-02-25T20:46:11.449347Z",
     "iopub.status.idle": "2022-02-25T20:46:11.463136Z",
     "shell.execute_reply": "2022-02-25T20:46:11.462167Z",
     "shell.execute_reply.started": "2022-02-25T20:46:11.449347Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from configuration import *\n",
    "# print(\"DATASET_SIZE\", DATASET_SIZE)\n",
    "# print(\"TRAIN_SIZE\", TRAIN_SIZE)\n",
    "# print(\"TEST_SIZE\", TEST_SIZE)\n",
    "\n",
    "# np.random.seed(SEED)\n",
    "# tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2287e1-7f38-4d9d-88aa-4b7162029b06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:11.465153Z",
     "iopub.status.busy": "2022-02-25T20:46:11.465153Z",
     "iopub.status.idle": "2022-02-25T20:46:11.479088Z",
     "shell.execute_reply": "2022-02-25T20:46:11.478138Z",
     "shell.execute_reply.started": "2022-02-25T20:46:11.465153Z"
    }
   },
   "outputs": [],
   "source": [
    "# from dataset_utils import create_image_loader\n",
    "\n",
    "# train_dataset = tf.data.Dataset.range(TRAIN_SIZE).shuffle(TRAIN_SIZE)\n",
    "# test_dataset = tf.data.Dataset.range(TEST_SIZE).shuffle(TEST_SIZE)\n",
    "\n",
    "# f2r_train_dataset = train_dataset.map(create_image_loader(2, 3, TRAIN_SIZES, \"train\"), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# f2r_train_dataset = f2r_train_dataset.batch(BATCH_SIZE)\n",
    "# f2r_test_dataset = test_dataset.map(create_image_loader(2, 3, TEST_SIZES, \"test\"))\n",
    "# f2r_test_dataset = f2r_test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# f2b_train_dataset = train_dataset.map(create_image_loader(2, 0, TRAIN_SIZES, \"train\"), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# f2b_train_dataset = f2b_train_dataset.batch(BATCH_SIZE)\n",
    "# f2b_test_dataset = test_dataset.map(create_image_loader(2, 0, TEST_SIZES, \"test\"))\n",
    "# f2b_test_dataset = f2b_test_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# f2l_train_dataset = train_dataset.map(create_image_loader(2, 1, TRAIN_SIZES, \"train\"), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# f2l_train_dataset = f2l_train_dataset.batch(BATCH_SIZE)\n",
    "# f2l_test_dataset = test_dataset.map(create_image_loader(2, 1, TEST_SIZES, \"test\"))\n",
    "# f2l_test_dataset = f2l_test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "604cad40-9946-42d4-8b2f-a85b89ec5173",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:11.479088Z",
     "iopub.status.busy": "2022-02-25T20:46:11.479088Z",
     "iopub.status.idle": "2022-02-25T20:46:11.495176Z",
     "shell.execute_reply": "2022-02-25T20:46:11.494230Z",
     "shell.execute_reply.started": "2022-02-25T20:46:11.479088Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pix2pix_model import Pix2PixModel\n",
    "\n",
    "# model = Pix2PixModel(f2r_train_dataset, f2r_test_dataset, \"front2right\", \"pix2pix-rpgmakerxp\")\n",
    "# model.load_generator()\n",
    "# model.load_discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa85b8e9-a0a7-40f5-b94a-cb35b42f58f5",
   "metadata": {},
   "source": [
    "## Generating fake/real images dataset\n",
    "\n",
    "This uses the previously loaded model to generate a dataset of real/fake images using some generator/discriminator from some previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "335967b1-8149-4970-bd59-b21e16771c98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:11.496203Z",
     "iopub.status.busy": "2022-02-25T20:46:11.496203Z",
     "iopub.status.idle": "2022-02-25T20:46:11.573155Z",
     "shell.execute_reply": "2022-02-25T20:46:11.573155Z",
     "shell.execute_reply.started": "2022-02-25T20:46:11.496203Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io_utils import delete_folder, ensure_folder_structure\n",
    "\n",
    "SOURCE_IMAGES_PATH = os.sep.join([TEMP_FOLDER, \"improving-pix2pix\", \"source\"])\n",
    "GENERATED_IMAGES_PATH = os.sep.join([TEMP_FOLDER, \"improving-pix2pix\", \"fake\"])\n",
    "TARGET_IMAGES_PATH = os.sep.join([TEMP_FOLDER, \"improving-pix2pix\", \"real\"])\n",
    "# delete_folder(SOURCE_IMAGES_PATH)\n",
    "# delete_folder(GENERATED_IMAGES_PATH)\n",
    "# delete_folder(TARGET_IMAGES_PATH)\n",
    "\n",
    "\n",
    "# for ds_name in [\"train\", \"test\"]:\n",
    "#     ensure_folder_structure(os.sep.join([SOURCE_IMAGES_PATH, ds_name]))\n",
    "#     ensure_folder_structure(os.sep.join([GENERATED_IMAGES_PATH, ds_name]))\n",
    "#     ensure_folder_structure(os.sep.join([TARGET_IMAGES_PATH, ds_name]))\n",
    "\n",
    "#     print(f\"Generating from {ds_name}\")\n",
    "#     dataset = model.train_ds if ds_name == \"train\" else model.test_ds\n",
    "    \n",
    "#     input_images = []\n",
    "#     target_images = []\n",
    "#     generated_images = []\n",
    "#     discriminated_target_images = []\n",
    "#     discriminated_generated_images = []\n",
    "\n",
    "#     for i, (input_image, target_image) in dataset.enumerate():\n",
    "#         # defines or creates the images\n",
    "#         input_images.append(input_image)\n",
    "#         target_images.append(target_image)\n",
    "#         generated_image = model.generator(input_image, training=True)\n",
    "#         generated_images.append(generated_image)\n",
    "#         discriminated_target_images.append(model.discriminator([input_image, target_image], training=True))\n",
    "#         discriminated_generated_images.append(model.discriminator([input_image, generated_image], training=True))\n",
    "       \n",
    "#         # save images to disk\n",
    "#         tf.keras.utils.save_img(f\"{os.sep.join([SOURCE_IMAGES_PATH, ds_name, str(i.numpy())])}.png\", input_image[0])\n",
    "#         tf.keras.utils.save_img(f\"{os.sep.join([TARGET_IMAGES_PATH, ds_name, str(i.numpy())])}.png\", target_image[0])\n",
    "#         tf.keras.utils.save_img(f\"{os.sep.join([GENERATED_IMAGES_PATH, ds_name, str(i.numpy())])}.png\", generated_image[0])\n",
    "        \n",
    "#         # show only the first 5 in the notebook\n",
    "#         if i < 5:\n",
    "#             plt.figure(figsize=(16, 16*5))\n",
    "#             for j, image in enumerate([input_image, target_image, generated_image]):\n",
    "#                 plt.subplot(1, 5, j+1)\n",
    "#                 plt.imshow(tf.squeeze(image) * 0.5 + 0.5, interpolation=\"nearest\")\n",
    "#                 plt.axis(\"off\")\n",
    "\n",
    "#             for j, image in enumerate([discriminated_target_images[-1], discriminated_generated_images[-1]]):\n",
    "#                 plt.subplot(1, 5, j+4)\n",
    "#                 plt.imshow(tf.squeeze(image) * 0.5 + 0.5, cmap=\"gray\", vmin=0.0, vmax=1.0, interpolation=\"nearest\")\n",
    "#                 plt.axis(\"off\")\n",
    "#             plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d64002-d822-44fb-a095-9fbba42791b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:11.575155Z",
     "iopub.status.busy": "2022-02-25T20:46:11.575155Z",
     "iopub.status.idle": "2022-02-25T20:46:11.984061Z",
     "shell.execute_reply": "2022-02-25T20:46:11.983064Z",
     "shell.execute_reply.started": "2022-02-25T20:46:11.575155Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, 64, 64, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " target_image (InputLayer)      [(None, 64, 64, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64, 64, 8)    0           ['input_image[0][0]',            \n",
      "                                                                  'target_image[0][0]']           \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 32, 32, 8)    1024        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " zero_padding2d (ZeroPadding2D)  (None, 34, 34, 8)   0           ['sequential[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 33, 33, 8)    264         ['zero_padding2d[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 33, 33, 8)   32          ['conv2d_1[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 33, 33, 8)    0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, 35, 35, 8)   0           ['leaky_re_lu_1[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 34, 34, 16)   528         ['zero_padding2d_1[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 34, 34, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 34, 34, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 34, 34, 16)   0           ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " zero_padding2d_2 (ZeroPadding2  (None, 36, 36, 16)  0           ['dropout[0][0]']                \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 33, 33, 16)   4112        ['zero_padding2d_2[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 33, 33, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 33, 33, 16)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " zero_padding2d_3 (ZeroPadding2  (None, 35, 35, 16)  0           ['leaky_re_lu_3[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 34, 34, 32)   2080        ['zero_padding2d_3[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 34, 34, 32)  128         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 34, 34, 32)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 34, 34, 32)   0           ['leaky_re_lu_4[0][0]']          \n",
      "                                                                                                  \n",
      " zero_padding2d_4 (ZeroPadding2  (None, 36, 36, 32)  0           ['dropout_1[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 33, 33, 32)   16416       ['zero_padding2d_4[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 33, 33, 32)  128         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 33, 33, 32)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " zero_padding2d_5 (ZeroPadding2  (None, 35, 35, 32)  0           ['leaky_re_lu_5[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 64)   32832       ['zero_padding2d_5[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 32, 32, 64)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 32, 32, 64)   0           ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " zero_padding2d_6 (ZeroPadding2  (None, 34, 34, 64)  0           ['dropout_2[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 64)   36928       ['zero_padding2d_6[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 32, 32, 64)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " zero_padding2d_7 (ZeroPadding2  (None, 34, 34, 64)  0           ['leaky_re_lu_7[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 31, 31, 128)  131072      ['zero_padding2d_7[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 31, 31, 128)  512        ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 31, 31, 128)  0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " zero_padding2d_8 (ZeroPadding2  (None, 33, 33, 128)  0          ['leaky_re_lu_8[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 30, 30, 1)    2049        ['zero_padding2d_8[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 228,745\n",
      "Trainable params: 228,025\n",
      "Non-trainable params: 720\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(layers.Conv2D(\n",
    "        filters,\n",
    "        size,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=initializer,\n",
    "        use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(layers.BatchNormalization())\n",
    "\n",
    "    result.add(layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def P2PDiscriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    input = layers.Input(shape=[IMG_SIZE, IMG_SIZE, OUTPUT_CHANNELS], name=\"input_image\")\n",
    "    target = layers.Input(shape=[IMG_SIZE, IMG_SIZE, OUTPUT_CHANNELS], name=\"target_image\")\n",
    "\n",
    "    x = layers.concatenate([input, target])  # (batch_size, 64, 64, channels*2)\n",
    "\n",
    "\n",
    "    down = downsample(8, 4, False)(x)  # (batch_size, 32, 32, 64)\n",
    "    \n",
    "    down = layers.ZeroPadding2D()(down)  # (batch_size, 34, 34, 64)\n",
    "    down = layers.Conv2D(8, 2, strides=1, kernel_initializer=initializer)(down)\n",
    "    down = layers.BatchNormalization()(down)\n",
    "    down = layers.LeakyReLU()(down)\n",
    "\n",
    "    down = layers.ZeroPadding2D()(down)  # (batch_size, 34, 34, 64)\n",
    "    down = layers.Conv2D(16, 2, strides=1, kernel_initializer=initializer)(down)\n",
    "    down = layers.BatchNormalization()(down)\n",
    "    down = layers.LeakyReLU()(down)\n",
    "    down = layers.Dropout(0.5)(down)\n",
    "    \n",
    "    down = layers.ZeroPadding2D()(down)  # (batch_size, 34, 34, 64)\n",
    "    down = layers.Conv2D(16, 4, strides=1, kernel_initializer=initializer)(down)\n",
    "    down = layers.BatchNormalization()(down)\n",
    "    down = layers.LeakyReLU()(down)\n",
    "    \n",
    "    down = layers.ZeroPadding2D()(down)  # (batch_size, 34, 34, 64)\n",
    "    down = layers.Conv2D(32, 2, strides=1, kernel_initializer=initializer)(down)\n",
    "    down = layers.BatchNormalization()(down)\n",
    "    down = layers.LeakyReLU()(down)\n",
    "    down = layers.Dropout(0.5)(down)\n",
    "    \n",
    "    down = layers.ZeroPadding2D()(down)  # (batch_size, 34, 34, 64)\n",
    "    down = layers.Conv2D(32, 4, strides=1, kernel_initializer=initializer)(down)\n",
    "    down = layers.BatchNormalization()(down)\n",
    "    down = layers.LeakyReLU()(down)\n",
    "    \n",
    "    down = layers.ZeroPadding2D()(down)  # (batch_size, 34, 34, 64)\n",
    "    down = layers.Conv2D(64, 4, strides=1, kernel_initializer=initializer)(down)\n",
    "    down = layers.BatchNormalization()(down)\n",
    "    down = layers.LeakyReLU()(down)\n",
    "    down = layers.Dropout(0.5)(down)\n",
    "    \n",
    "    down = layers.ZeroPadding2D()(down)  # (batch_size, 34, 34, 64)\n",
    "    down = layers.Conv2D(64, 3, strides=1, kernel_initializer=initializer)(down)\n",
    "    down = layers.BatchNormalization()(down)\n",
    "    down = layers.LeakyReLU()(down)\n",
    "    \n",
    "    zero_pad1 = layers.ZeroPadding2D()(down)  # (batch_size, 34, 34, 64)\n",
    "    conv = layers.Conv2D(128, 4, strides=1,\n",
    "                                 use_bias=False,\n",
    "                                kernel_initializer=initializer)(zero_pad1)  # (batch_size, 31, 31, 128)\n",
    "    batchnorm1 = layers.BatchNormalization()(conv)\n",
    "    leaky_relu = layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 128)\n",
    "    last = layers.Conv2D(1, 4, strides=1,\n",
    "                            kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=[input, target], outputs=last)\n",
    "\n",
    "\n",
    "\n",
    "candidate_discriminator = P2PDiscriminator()\n",
    "candidate_discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39f309f7-e066-430e-9b4d-9bf347ff66ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:11.986055Z",
     "iopub.status.busy": "2022-02-25T20:46:11.985060Z",
     "iopub.status.idle": "2022-02-25T20:46:12.434723Z",
     "shell.execute_reply": "2022-02-25T20:46:12.433724Z",
     "shell.execute_reply.started": "2022-02-25T20:46:11.986055Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, 64, 64, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " target_image (InputLayer)      [(None, 64, 64, 4)]  0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64, 64, 8)    0           ['input_image[0][0]',            \n",
      "                                                                  'target_image[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 32, 32, 64)   8192        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)      (None, 16, 16, 128)  131584      ['sequential_1[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)      (None, 8, 8, 256)    525312      ['sequential_2[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_4 (Sequential)      (None, 4, 4, 512)    2099200     ['sequential_3[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_5 (Sequential)      (None, 2, 2, 512)    4196352     ['sequential_4[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)      (None, 1, 1, 512)    4196352     ['sequential_5[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_7 (Sequential)      (None, 2, 2, 512)    4196352     ['sequential_6[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 2, 2, 1024)   0           ['sequential_7[0][0]',           \n",
      "                                                                  'sequential_5[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_8 (Sequential)      (None, 4, 4, 512)    8390656     ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 4, 4, 1024)   0           ['sequential_8[0][0]',           \n",
      "                                                                  'sequential_4[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_9 (Sequential)      (None, 8, 8, 512)    8390656     ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 8, 8, 768)    0           ['sequential_9[0][0]',           \n",
      "                                                                  'sequential_3[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_10 (Sequential)     (None, 16, 16, 512)  6293504     ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 16, 16, 640)  0           ['sequential_10[0][0]',          \n",
      "                                                                  'sequential_2[0][0]']           \n",
      "                                                                                                  \n",
      " sequential_11 (Sequential)     (None, 32, 32, 256)  2622464     ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 32, 32, 320)  0           ['sequential_11[0][0]',          \n",
      "                                                                  'sequential_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 30, 30, 1)    2881        ['concatenate_6[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 41,053,505\n",
      "Trainable params: 41,045,057\n",
      "Non-trainable params: 8,448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                padding=\"same\",\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False))\n",
    "\n",
    "    result.add(layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(layers.Dropout(0.5))\n",
    "\n",
    "    result.add(layers.ReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def UnetDiscriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    input = layers.Input(shape=[IMG_SIZE, IMG_SIZE, OUTPUT_CHANNELS], name=\"input_image\")\n",
    "    target = layers.Input(shape=[IMG_SIZE, IMG_SIZE, OUTPUT_CHANNELS], name=\"target_image\")\n",
    "\n",
    "    inputs = layers.concatenate([input, target])  # (batch_size, 64, 64, channels*2)\n",
    "    \n",
    "    down_stack = [\n",
    "        downsample(64, 4, apply_batchnorm=False),  # (batch_size, 32, 32, 64)\n",
    "        downsample(128, 4),  # (batch_size, 16, 16, 128)\n",
    "        downsample(256, 4),  # (batch_size, 8, 8, 256)\n",
    "        downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "        downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "        downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
    "    ]\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "        upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "        upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "        upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "    ]\n",
    "\n",
    "    last = layers.Conv2D(1, 3,\n",
    "                                     strides=1,\n",
    "                                     # padding=\"same\",\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     activation=\"tanh\"\n",
    "                                 )  # (batch_size, 32, 32, 4)\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # downsampling e adicionando as skip-connections\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # camadas de upsampling e skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=[input, target], outputs=x)\n",
    "\n",
    "\n",
    "\n",
    "candidate_discriminator = UnetDiscriminator()\n",
    "candidate_discriminator.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d4f653d-9615-45e8-a851-d1286fc70034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:12.435720Z",
     "iopub.status.busy": "2022-02-25T20:46:12.435720Z",
     "iopub.status.idle": "2022-02-25T20:46:12.465739Z",
     "shell.execute_reply": "2022-02-25T20:46:12.464743Z",
     "shell.execute_reply.started": "2022-02-25T20:46:12.435720Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from custom_layers import MaxPoolingWithArgmax2D, MaxUnpooling2D\n",
    "\n",
    "\n",
    "def downsample_segnet(inputs, filters, size, convolution_steps=2, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    x = inputs\n",
    "    for i in range(convolution_steps):\n",
    "        x = layers.Conv2D(\n",
    "            filters,\n",
    "            size,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=initializer)(x)\n",
    "\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        \n",
    "    \n",
    "    x, indices = MaxPoolingWithArgmax2D(pool_size=2)(x)\n",
    "    # x, indices = tf.raw_ops.MaxPoolWithArgmax(input=x, ksize=[size, size], strides=[2, 2], padding=\"valid\", Targmax=tf.dtypes.int32)\n",
    "    return x, indices\n",
    "    \n",
    "    \n",
    "def upsample_segnet(inputs, filters, size, convolution_steps=2, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    x, indices = inputs\n",
    "    # print(f\"x is {x}\")\n",
    "    # print(f\"indices is {indices}\")\n",
    "    x = MaxUnpooling2D(size=2)([x, indices])\n",
    "    # x = tfa.layers.MaxUnpooling2D(pool_size=size)([x, indices])\n",
    "    \n",
    "    for i in range(convolution_steps):\n",
    "        x = layers.Conv2D(filters, size, strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_initializer=initializer\n",
    "                         )(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "    if apply_dropout:\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # print(f\"shape of x after upsample: {tf.shape(x)}\")\n",
    "    return x\n",
    "\n",
    "    \n",
    "def SegNetDiscriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    input = layers.Input(shape=[IMG_SIZE, IMG_SIZE, OUTPUT_CHANNELS], name=\"input_image\")\n",
    "    target = layers.Input(shape=[IMG_SIZE, IMG_SIZE, OUTPUT_CHANNELS], name=\"target_image\")\n",
    "\n",
    "    inputs = layers.concatenate([input, target])  # (batch_size, 64, 64, channels*2)\n",
    "    \n",
    "    output = downsample_segnet(inputs, 64, 4, 2),  # (batch_size, 32, 32, 64)\n",
    "    x, indices0 = output[0][0], output[0][1]\n",
    "    output = downsample_segnet(x, 128, 4, 1),  # (batch_size, 16, 16, 128)\n",
    "    x, indices1 = output[0][0], output[0][1]\n",
    "    output = downsample_segnet(x, 256, 4, 2),  # (batch_size, 8, 8, 256)\n",
    "    x, indices2 = output[0][0], output[0][1]\n",
    "    output = downsample_segnet(x, 512, 4, 1),  # (batch_size, 4, 4, 512)\n",
    "    x, indices3 = output[0][0], output[0][1]\n",
    "    # output = downsample_segnet(x, 512, 4, 3),  # (batch_size, 2, 2, 512)\n",
    "    # x, indices4 = output[0][0], output[0][1]\n",
    "\n",
    "    # x = upsample_segnet([x, indices4], 512, 4, 3, apply_dropout=True),  # (batch_size, 4, 4, 512)\n",
    "    # # print()\n",
    "    x = upsample_segnet([x, indices3], 256, 4, 3, apply_dropout=True),  # (batch_size, 8, 8, 512)\n",
    "    # print()\n",
    "    x = upsample_segnet([x, indices2], 128, 4, 3),  # (batch_size, 16, 16, 512)\n",
    "    # print()\n",
    "    x = upsample_segnet([x, indices1], 64, 4, 2),  # (batch_size, 32, 32, 256)\n",
    "\n",
    "    last = layers.Conv2D(1, 3, strides=1,\n",
    "                            padding=\"valid\",\n",
    "                            kernel_initializer=initializer,\n",
    "                         activation=\"tanh\")\n",
    "    # last = layers.Conv2D(1, 3,\n",
    "    #                          strides=1,\n",
    "    #                          # padding=\"same\",\n",
    "    #                          kernel_initializer=initializer,\n",
    "    #                          activation=\"tanh\"\n",
    "    #                      )  # (batch_size, 32, 32, 4)\n",
    "\n",
    "    \n",
    "    x = last(x[0])\n",
    "\n",
    "    return tf.keras.Model(inputs=[input, target], outputs=x)\n",
    "\n",
    "\n",
    "\n",
    "# candidate_discriminator = SegNetDiscriminator()\n",
    "# candidate_discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec9c58b3-23b4-4e70-9efd-1d72d2326c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:12.466736Z",
     "iopub.status.busy": "2022-02-25T20:46:12.466736Z",
     "iopub.status.idle": "2022-02-25T20:46:12.481700Z",
     "shell.execute_reply": "2022-02-25T20:46:12.480701Z",
     "shell.execute_reply.started": "2022-02-25T20:46:12.466736Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1/127.5, offset=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a02810eb-d3c3-46d8-af84-b04d21a4607b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:12.482694Z",
     "iopub.status.busy": "2022-02-25T20:46:12.482694Z",
     "iopub.status.idle": "2022-02-25T20:46:12.746222Z",
     "shell.execute_reply": "2022-02-25T20:46:12.745263Z",
     "shell.execute_reply.started": "2022-02-25T20:46:12.482694Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 250 files belonging to 1 classes.\n",
      "Found 250 files belonging to 1 classes.\n",
      "Found 250 files belonging to 1 classes.\n",
      "Found 44 files belonging to 1 classes.\n",
      "Found 44 files belonging to 1 classes.\n",
      "Found 44 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "X = {\"train\": None, \"test\": None}\n",
    "\n",
    "for ds_name in [\"train\", \"test\"]:\n",
    "    source_images = tf.keras.utils.image_dataset_from_directory(os.sep.join([SOURCE_IMAGES_PATH, ds_name]), shuffle=False, labels=None, image_size=(IMG_SIZE, IMG_SIZE), color_mode=\"rgba\").map(lambda img: normalization_layer(img)).unbatch()\n",
    "    target_images = tf.keras.utils.image_dataset_from_directory(os.sep.join([TARGET_IMAGES_PATH, ds_name]), shuffle=False, labels=None, image_size=(IMG_SIZE, IMG_SIZE), color_mode=\"rgba\").map(lambda img: normalization_layer(img)).unbatch()\n",
    "    generated_images = tf.keras.utils.image_dataset_from_directory(os.sep.join([GENERATED_IMAGES_PATH, ds_name]), shuffle=False, labels=None, image_size=(IMG_SIZE, IMG_SIZE), color_mode=\"rgba\").map(lambda img: normalization_layer(img)).unbatch()\n",
    "    \n",
    "    folder = os.sep.join([\".\", SOURCE_IMAGES_PATH, ds_name])\n",
    "    number_of_examples = len([name for name in os.listdir(folder) if os.path.isfile(os.sep.join([folder, name]))])\n",
    "\n",
    "    y_real = tf.ones((number_of_examples, 30, 30, 1))\n",
    "    y_fake = y_real * 0.\n",
    "    y_real = tf.data.Dataset.from_tensor_slices(y_real)#.repeat(number_of_examples)\n",
    "    y_fake = tf.data.Dataset.from_tensor_slices(y_fake)#.repeat(number_of_examples)\n",
    "\n",
    "    X_real = tf.data.Dataset.zip(((source_images, target_images), y_real))\n",
    "    X_fake = tf.data.Dataset.zip(((source_images, generated_images), y_fake))\n",
    "\n",
    "    X[ds_name] = X_real.concatenate(X_fake).shuffle(1000).batch(16)\n",
    "    \n",
    "X_train = X[\"train\"]\n",
    "X_test = X[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b7e236-e099-47bd-9da1-bf85eab26b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:46:12.747206Z",
     "iopub.status.busy": "2022-02-25T20:46:12.747206Z",
     "iopub.status.idle": "2022-02-25T20:54:44.807974Z",
     "shell.execute_reply": "2022-02-25T20:54:44.807004Z",
     "shell.execute_reply.started": "2022-02-25T20:46:12.747206Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "6/6 [==============================] - 7s 512ms/step - loss: 0.7406 - accuracy: 0.4954\n",
      "Epoch 2/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.7298 - accuracy: 0.5208\n",
      "Epoch 3/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.6234 - accuracy: 0.6514\n",
      "Epoch 4/200\n",
      "6/6 [==============================] - 2s 204ms/step - loss: 0.4335 - accuracy: 0.8543\n",
      "Epoch 5/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3780 - accuracy: 0.9207\n",
      "Epoch 6/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.4183 - accuracy: 0.8687\n",
      "Epoch 7/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3662 - accuracy: 0.9356\n",
      "Epoch 8/200\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 0.3550 - accuracy: 0.9533\n",
      "Epoch 9/200\n",
      "6/6 [==============================] - 3s 451ms/step - loss: 0.3702 - accuracy: 0.9434\n",
      "Epoch 10/200\n",
      "6/6 [==============================] - 3s 583ms/step - loss: 0.3621 - accuracy: 0.9408\n",
      "Epoch 11/200\n",
      "6/6 [==============================] - 3s 613ms/step - loss: 0.3665 - accuracy: 0.9297\n",
      "Epoch 12/200\n",
      "6/6 [==============================] - 3s 201ms/step - loss: 0.3529 - accuracy: 0.9606\n",
      "Epoch 13/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3820 - accuracy: 0.9314\n",
      "Epoch 14/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3375 - accuracy: 0.9742\n",
      "Epoch 15/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3319 - accuracy: 0.9850\n",
      "Epoch 16/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3265 - accuracy: 0.9804\n",
      "Epoch 17/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3559 - accuracy: 0.9585\n",
      "Epoch 18/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3603 - accuracy: 0.9432\n",
      "Epoch 19/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3281 - accuracy: 0.9817\n",
      "Epoch 20/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3271 - accuracy: 0.9810\n",
      "Epoch 21/200\n",
      "6/6 [==============================] - 2s 205ms/step - loss: 0.3163 - accuracy: 0.9997\n",
      "Epoch 22/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3265 - accuracy: 0.9854\n",
      "Epoch 23/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3226 - accuracy: 0.9869\n",
      "Epoch 24/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3155 - accuracy: 0.9992\n",
      "Epoch 25/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3167 - accuracy: 0.9997\n",
      "Epoch 26/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3150 - accuracy: 0.9998\n",
      "Epoch 27/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3262 - accuracy: 0.9809\n",
      "Epoch 28/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3143 - accuracy: 0.9993\n",
      "Epoch 29/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3207 - accuracy: 0.9931\n",
      "Epoch 30/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3146 - accuracy: 0.9986\n",
      "Epoch 31/200\n",
      "6/6 [==============================] - 2s 200ms/step - loss: 0.3135 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3277 - accuracy: 0.9920\n",
      "Epoch 33/200\n",
      "6/6 [==============================] - 2s 203ms/step - loss: 0.3135 - accuracy: 0.9998\n",
      "Epoch 34/200\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 0.3201 - accuracy: 0.9941\n",
      "Epoch 35/200\n",
      "6/6 [==============================] - 3s 580ms/step - loss: 0.3136 - accuracy: 0.9997\n",
      "Epoch 36/200\n",
      "6/6 [==============================] - 3s 588ms/step - loss: 0.3321 - accuracy: 0.9780\n",
      "Epoch 37/200\n",
      "6/6 [==============================] - 3s 604ms/step - loss: 0.3261 - accuracy: 0.9815\n",
      "Epoch 38/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3160 - accuracy: 0.9968\n",
      "Epoch 39/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3183 - accuracy: 0.9984\n",
      "Epoch 40/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3180 - accuracy: 0.9992\n",
      "Epoch 41/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3137 - accuracy: 0.9998\n",
      "Epoch 42/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3200 - accuracy: 0.9910\n",
      "Epoch 43/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3184 - accuracy: 0.9897\n",
      "Epoch 44/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3229 - accuracy: 0.9852\n",
      "Epoch 45/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3189 - accuracy: 0.9966\n",
      "Epoch 46/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3141 - accuracy: 0.9992\n",
      "Epoch 47/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3159 - accuracy: 0.9982\n",
      "Epoch 48/200\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 0.3207 - accuracy: 0.9908\n",
      "Epoch 49/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3134 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3168 - accuracy: 0.9944\n",
      "Epoch 52/200\n",
      "6/6 [==============================] - 2s 202ms/step - loss: 0.3134 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3138 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3142 - accuracy: 0.9990\n",
      "Epoch 55/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3147 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3134 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "6/6 [==============================] - 2s 201ms/step - loss: 0.3150 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3151 - accuracy: 0.9962\n",
      "Epoch 59/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3135 - accuracy: 0.9999\n",
      "Epoch 60/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3152 - accuracy: 0.9997\n",
      "Epoch 61/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3137 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "6/6 [==============================] - 3s 602ms/step - loss: 0.3149 - accuracy: 0.9979\n",
      "Epoch 63/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "6/6 [==============================] - 2s 202ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3136 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3135 - accuracy: 0.9998\n",
      "Epoch 67/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3143 - accuracy: 0.9989\n",
      "Epoch 68/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3310 - accuracy: 0.9857\n",
      "Epoch 70/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3217 - accuracy: 0.9884\n",
      "Epoch 71/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3171 - accuracy: 0.9937\n",
      "Epoch 72/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3343 - accuracy: 0.9709\n",
      "Epoch 73/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3508 - accuracy: 0.9644\n",
      "Epoch 74/200\n",
      "6/6 [==============================] - 2s 203ms/step - loss: 0.3300 - accuracy: 0.9849\n",
      "Epoch 75/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3523 - accuracy: 0.9555\n",
      "Epoch 76/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3329 - accuracy: 0.9754\n",
      "Epoch 77/200\n",
      "6/6 [==============================] - 2s 204ms/step - loss: 0.3237 - accuracy: 0.9898\n",
      "Epoch 78/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3194 - accuracy: 0.9957\n",
      "Epoch 79/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3151 - accuracy: 0.9997\n",
      "Epoch 80/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3136 - accuracy: 0.9998\n",
      "Epoch 81/200\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 0.3202 - accuracy: 0.9966\n",
      "Epoch 82/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3134 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "6/6 [==============================] - 2s 202ms/step - loss: 0.3178 - accuracy: 0.9975\n",
      "Epoch 84/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3155 - accuracy: 0.9959\n",
      "Epoch 85/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3136 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "6/6 [==============================] - 2s 202ms/step - loss: 0.3164 - accuracy: 0.9952\n",
      "Epoch 87/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "6/6 [==============================] - 2s 202ms/step - loss: 0.3134 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "6/6 [==============================] - 1s 200ms/step - loss: 0.3170 - accuracy: 0.9936\n",
      "Epoch 90/200\n",
      "6/6 [==============================] - 2s 203ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3266 - accuracy: 0.9881\n",
      "Epoch 92/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3139 - accuracy: 0.9990\n",
      "Epoch 93/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3170 - accuracy: 0.9945\n",
      "Epoch 94/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3140 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 0.3266 - accuracy: 0.9887\n",
      "Epoch 96/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3143 - accuracy: 0.9997\n",
      "Epoch 97/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3149 - accuracy: 0.9980\n",
      "Epoch 98/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3135 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3135 - accuracy: 0.9999\n",
      "Epoch 100/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3157 - accuracy: 0.9968\n",
      "Epoch 101/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3198 - accuracy: 0.9894\n",
      "Epoch 103/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3240 - accuracy: 0.9924\n",
      "Epoch 104/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3163 - accuracy: 0.9995\n",
      "Epoch 105/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3168 - accuracy: 0.9951\n",
      "Epoch 106/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3135 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3134 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 0.3277 - accuracy: 0.9847\n",
      "Epoch 109/200\n",
      "6/6 [==============================] - 1s 205ms/step - loss: 0.3136 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "6/6 [==============================] - 2s 204ms/step - loss: 0.3140 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.3138 - accuracy: 0.9993\n",
      "Epoch 112/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3136 - accuracy: 0.9999\n",
      "Epoch 113/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3249 - accuracy: 0.9849\n",
      "Epoch 115/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3259 - accuracy: 0.9909\n",
      "Epoch 117/200\n",
      "6/6 [==============================] - 1s 202ms/step - loss: 0.3135 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "6/6 [==============================] - 2s 208ms/step - loss: 0.3176 - accuracy: 0.9973\n",
      "Epoch 120/200\n",
      "6/6 [==============================] - 2s 396ms/step - loss: 0.3169 - accuracy: 0.9939\n",
      "Epoch 121/200\n",
      "6/6 [==============================] - 3s 580ms/step - loss: 0.3149 - accuracy: 0.9971\n",
      "Epoch 122/200\n",
      "6/6 [==============================] - 5s 595ms/step - loss: 0.3142 - accuracy: 0.9984\n",
      "Epoch 123/200\n",
      "6/6 [==============================] - 3s 569ms/step - loss: 0.3140 - accuracy: 0.9999\n",
      "Epoch 124/200\n",
      "6/6 [==============================] - 3s 607ms/step - loss: 0.3203 - accuracy: 0.9903\n",
      "Epoch 125/200\n",
      "6/6 [==============================] - 5s 612ms/step - loss: 0.3136 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "6/6 [==============================] - 3s 629ms/step - loss: 0.3147 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "6/6 [==============================] - 3s 588ms/step - loss: 0.3140 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "6/6 [==============================] - 3s 582ms/step - loss: 0.3145 - accuracy: 0.9999\n",
      "Epoch 129/200\n",
      "6/6 [==============================] - 5s 582ms/step - loss: 0.3134 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "6/6 [==============================] - 3s 578ms/step - loss: 0.3162 - accuracy: 0.9952\n",
      "Epoch 131/200\n",
      "6/6 [==============================] - 3s 601ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "6/6 [==============================] - 2s 204ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3134 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "6/6 [==============================] - 1s 201ms/step - loss: 0.3156 - accuracy: 0.9998\n",
      "Epoch 135/200\n",
      "6/6 [==============================] - 2s 218ms/step - loss: 0.3134 - accuracy: 0.9999\n",
      "Epoch 136/200\n",
      "6/6 [==============================] - 5s 585ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "6/6 [==============================] - 3s 601ms/step - loss: 0.3146 - accuracy: 0.9984\n",
      "Epoch 138/200\n",
      "6/6 [==============================] - 3s 587ms/step - loss: 0.3175 - accuracy: 0.9925\n",
      "Epoch 139/200\n",
      "6/6 [==============================] - 3s 199ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "6/6 [==============================] - 2s 206ms/step - loss: 0.3141 - accuracy: 0.9997\n",
      "Epoch 141/200\n",
      "6/6 [==============================] - 5s 581ms/step - loss: 0.3137 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "6/6 [==============================] - 3s 586ms/step - loss: 0.3167 - accuracy: 0.9986\n",
      "Epoch 143/200\n",
      "6/6 [==============================] - 3s 587ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "6/6 [==============================] - 5s 588ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "6/6 [==============================] - 3s 590ms/step - loss: 0.3138 - accuracy: 0.9995\n",
      "Epoch 146/200\n",
      "6/6 [==============================] - 5s 586ms/step - loss: 0.3135 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "6/6 [==============================] - 3s 592ms/step - loss: 0.3140 - accuracy: 0.9990\n",
      "Epoch 148/200\n",
      "6/6 [==============================] - 3s 588ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "6/6 [==============================] - 5s 586ms/step - loss: 0.3134 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "6/6 [==============================] - 3s 586ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "6/6 [==============================] - 3s 583ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "6/6 [==============================] - 5s 589ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "6/6 [==============================] - 3s 588ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "6/6 [==============================] - 3s 584ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "6/6 [==============================] - 5s 587ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "6/6 [==============================] - 3s 583ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "6/6 [==============================] - 3s 590ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "6/6 [==============================] - 3s 588ms/step - loss: 0.3134 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "6/6 [==============================] - 3s 583ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "6/6 [==============================] - 3s 589ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "6/6 [==============================] - 5s 585ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "6/6 [==============================] - 3s 584ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "6/6 [==============================] - 3s 586ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "6/6 [==============================] - 5s 586ms/step - loss: 0.3134 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "6/6 [==============================] - 3s 593ms/step - loss: 0.3135 - accuracy: 0.9998\n",
      "Epoch 166/200\n",
      "6/6 [==============================] - 3s 593ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "6/6 [==============================] - 5s 610ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "6/6 [==============================] - 3s 582ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "6/6 [==============================] - 5s 590ms/step - loss: 0.3245 - accuracy: 0.9844\n",
      "Epoch 170/200\n",
      "6/6 [==============================] - 3s 602ms/step - loss: 0.3172 - accuracy: 0.9980\n",
      "Epoch 171/200\n",
      "6/6 [==============================] - 3s 598ms/step - loss: 0.3136 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "6/6 [==============================] - 5s 601ms/step - loss: 0.3135 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "6/6 [==============================] - 3s 631ms/step - loss: 0.3210 - accuracy: 0.9912\n",
      "Epoch 174/200\n",
      "6/6 [==============================] - 4s 638ms/step - loss: 0.3137 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "6/6 [==============================] - 5s 600ms/step - loss: 0.3286 - accuracy: 0.9875\n",
      "Epoch 176/200\n",
      "6/6 [==============================] - 3s 600ms/step - loss: 0.3161 - accuracy: 0.9974\n",
      "Epoch 177/200\n",
      "6/6 [==============================] - 3s 595ms/step - loss: 0.3146 - accuracy: 0.9978\n",
      "Epoch 178/200\n",
      "6/6 [==============================] - 5s 588ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "6/6 [==============================] - 3s 595ms/step - loss: 0.3139 - accuracy: 0.9989\n",
      "Epoch 180/200\n",
      "6/6 [==============================] - 5s 590ms/step - loss: 0.3136 - accuracy: 0.9998\n",
      "Epoch 181/200\n",
      "6/6 [==============================] - 3s 581ms/step - loss: 0.3137 - accuracy: 0.9996\n",
      "Epoch 182/200\n",
      "6/6 [==============================] - 3s 591ms/step - loss: 0.3175 - accuracy: 0.9934\n",
      "Epoch 183/200\n",
      "6/6 [==============================] - 3s 588ms/step - loss: 0.3136 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "6/6 [==============================] - 5s 582ms/step - loss: 0.3269 - accuracy: 0.9871\n",
      "Epoch 185/200\n",
      "6/6 [==============================] - 3s 582ms/step - loss: 0.3357 - accuracy: 0.9764\n",
      "Epoch 186/200\n",
      "6/6 [==============================] - 5s 587ms/step - loss: 0.3173 - accuracy: 0.9927\n",
      "Epoch 187/200\n",
      "6/6 [==============================] - 3s 585ms/step - loss: 0.3167 - accuracy: 0.9938\n",
      "Epoch 188/200\n",
      "6/6 [==============================] - 3s 587ms/step - loss: 0.3342 - accuracy: 0.9785\n",
      "Epoch 189/200\n",
      "6/6 [==============================] - 5s 585ms/step - loss: 0.3206 - accuracy: 0.9931\n",
      "Epoch 190/200\n",
      "6/6 [==============================] - 3s 583ms/step - loss: 0.3141 - accuracy: 0.9982\n",
      "Epoch 191/200\n",
      "6/6 [==============================] - 3s 586ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "6/6 [==============================] - 5s 589ms/step - loss: 0.3186 - accuracy: 0.9926\n",
      "Epoch 193/200\n",
      "6/6 [==============================] - 3s 587ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "6/6 [==============================] - 5s 585ms/step - loss: 0.3137 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "6/6 [==============================] - 3s 589ms/step - loss: 0.3202 - accuracy: 0.9958\n",
      "Epoch 196/200\n",
      "6/6 [==============================] - 3s 597ms/step - loss: 0.3133 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "6/6 [==============================] - 5s 596ms/step - loss: 0.3142 - accuracy: 0.9987\n",
      "Epoch 198/200\n",
      "6/6 [==============================] - 3s 596ms/step - loss: 0.3135 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "6/6 [==============================] - 3s 592ms/step - loss: 0.3134 - accuracy: 0.9999\n",
      "Epoch 200/200\n",
      "6/6 [==============================] - 5s 590ms/step - loss: 0.3134 - accuracy: 0.9998\n"
     ]
    }
   ],
   "source": [
    "# def scheduler(epoch, lr):\n",
    "#     if epoch < 80:\n",
    "#         return lr\n",
    "#     else:\n",
    "#         return lr * tf.math.exp(-0.1)\n",
    "# callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "\n",
    "candidate_discriminator.compile(\n",
    "    tf.keras.optimizers.Adam(0.0002, beta_1=0.9),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "epochs = 200\n",
    "history = candidate_discriminator.fit(\n",
    "    X_test,\n",
    "    # X_train,\n",
    "    # validation_data=X_test,\n",
    "    # callbacks=[callback],\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7b5eb7d-0088-449f-8f2f-27284ab6eb85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T20:54:44.810966Z",
     "iopub.status.busy": "2022-02-25T20:54:44.810966Z",
     "iopub.status.idle": "2022-02-25T20:54:45.106178Z",
     "shell.execute_reply": "2022-02-25T20:54:45.105206Z",
     "shell.execute_reply.started": "2022-02-25T20:54:44.810966Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8*2, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "# plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Accuracy')\n",
    "plt.ylim(ymin=0, ymax=1) \n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "# plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Loss')\n",
    "plt.ylim(ymin=0, ymax=1) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
